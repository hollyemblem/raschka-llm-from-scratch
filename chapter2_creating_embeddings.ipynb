{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXM1C+JmGz8SN9bmS0ryvK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hollyemblem/raschka-llm-from-scratch/blob/chapter-2-embeddings/chapter2_creating_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 2\n",
        "\n",
        "#### Creating Token Embeddings\n",
        "We initialise embeddings - which is the process of converting token IDs into vector representations - with random values. In later chapters (Chapter 5) we will optimise the embedding weights..\n",
        "\n",
        "Raschka: \"Embedding layers perform a lookup operation, retrieving the embedding vector corresponding to the token ID from the embedding layer’s weight matrix.\"\n",
        "\n",
        "Me: Think about this as; embedding layers store a trainable matrix (with backprop for example) where each row is a token embedding. If we have a token ID, we can perform a look up and retrieve the corresponding row, which is the embedding vector.\n"
      ],
      "metadata": {
        "id": "vIkaiF70JqYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "4d0v9QAeJ8JT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##A toy example -  four input tokens with IDs 2, 3, 5, and 1\n",
        "\n",
        "input_ids = torch.tensor([2, 3, 5, 1])"
      ],
      "metadata": {
        "id": "8pI80t23KOUc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6 #This is different to our BPE vocab size of 50k+!\n",
        "output_dim = 3 #GPT-3's embedding size is  12,288 dimensions"
      ],
      "metadata": {
        "id": "_H6zwgMSKSbc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Instantiating a embedding layer\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhkGam54Kdi4",
        "outputId": "45aa3d66-2475-42ca-b6c8-e30d6adb55f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6x3 Matrix:\n",
        "There is one row for each of the six possible tokens in the vocabulary,\n",
        "and there is one column for each of the three embedding dimensions.\n",
        "\n"
      ],
      "metadata": {
        "id": "YbDlG00EKmOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, let’s apply it to a token ID to obtain the embedding vector:\n",
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZBWQ39XKo80",
        "outputId": "9cbfe099-9a47-4d14-d415-45ad50dd54ae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can find the above value in the fourth row of the matrix, or row 3 due to 0 numbering. This embedding layer is therefore a lookup which means we can retrieve rows from the embedding layer's weight matrix, with a token ID (in this instance, 3)\n",
        "\n",
        " In other words, the embedding layer is essentially a lookup operation that retrieves rows from the embedding layer’s weight matrix via a token ID.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0HaVAp-FLE4a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding one-hot encoding approach\n",
        "\"For those who are familiar with one-hot encoding, the embedding layer approach described here is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully connected layer...\n",
        "\n",
        "Because the embedding layer is just a more efficient implementation equivalent to the one-hot encoding and matrix-multiplication approach, it can be seen as a neural network layer that can be optimized via backpropagation.\"\n",
        "\n",
        "More detailed guidance available [here](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/03_bonus_embedding-vs-matmul/embeddings-and-linear-layers.ipynb)"
      ],
      "metadata": {
        "id": "fSrAoiUmLcOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F86cQLaALtRK",
        "outputId": "53205cec-6cd2-42a3-d312-8b3f29fbb295"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding Word Positions\n",
        "\n",
        "\"The way the previously introduced embedding layer works is that the same token ID always gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence\"\n",
        "\n",
        "But...Attention doesn't have a notion of position or order of a sequence. So while the above approach is quite deterministic and good for reproducibility, we need a method of injecting some positioning logic into the LLM."
      ],
      "metadata": {
        "id": "e9d8ag_PNl7K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4K_jGFt8Lz_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}