{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh1aMaTd/+z/ojG2Aa8laZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hollyemblem/raschka-llm-from-scratch/blob/main/chapter4_implementing_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding LLM Architecture\n",
        "\n",
        "A high-level view of the architecture required for predicting one word (token) at a time.\n",
        "\n",
        "Text -> Tokenized text -> Embeddings -> One or more Transformer blocks w/ multi-headed attention -> Output layers"
      ],
      "metadata": {
        "id": "FtqHdMex-EuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter Size and Changes\n",
        "\n",
        "In the previous chapter notebooks, we had small embedding dimensions so we could manually calculate the components of things like attention mechanisms. Now, we're scaling up to the small GPT model of 124 million parameters like [the GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (note, they originally state 117 million, but this was corrected).\n",
        "\n",
        "#### What are parameters?\n",
        "Parameters are trainable weights that are adjusted and optimised during training to minimise the loss function.\n",
        "\n",
        "#### How do we calculate the number of parameters?\n",
        "\n",
        "Imagine we have a neural network with a 2048 x 2048 trainable weight matrix. Every value in this matrix is a parameter, so we have 2048 x 2048 = 4,194,304 parameters."
      ],
      "metadata": {
        "id": "JDFUbKjH-yOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT Config\n"
      ],
      "metadata": {
        "id": "OzA_MA4h_jlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,          # Embedding dimension\n",
        "    \"n_heads\": 12,           # Number of attention heads\n",
        "    \"n_layers\": 12,          # Number of layers\n",
        "    \"drop_rate\": 0.1,        # Dropout rate\n",
        "    \"qkv_bias\": False        # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "WBUwnSRD-HNr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Implementing a placeholder GPT class\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        '''the embedding layer is essentially a lookup operation that retrieves rows from the embedding layer’s weight matrix via a token ID.'''\n",
        "\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "        self.trf_blocks = nn.Sequential(               #1 Transformer block placeholder\n",
        "            *[DummyTransformerBlock(cfg)               #1 Transformer block placeholder\n",
        "              for _ in range(cfg[\"n_layers\"])]         #1 Transformer block placeholder\n",
        "        )                                              #1 Transformer block placeholder\n",
        "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])     #2 Layer norm placeholder\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False #take an input of 768 and output 50257\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(\n",
        "            torch.arange(seq_len, device=in_idx.device)\n",
        "        )\n",
        "        x = tok_embeds + pos_embeds\n",
        "        print(\"Hidden representation shape:\", x.shape)  # ← THIS IS 768\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):    #3 placeholder class\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):     #4 placeholder forward pass\n",
        "        '''\n",
        "        forward method describes the data flow through the model: it computes token and positional embeddings for the input indices,\n",
        "        applies dropout, processes the data through the transformer blocks, applies normalization, and finally produces logits with the linear output layer.\n",
        "        '''\n",
        "        return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):           #5\n",
        "    def __init__(self, normalized_shape, eps=1e-5):    #6 placeholder layernorm\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ],
      "metadata": {
        "id": "lMkbFouJ_k0o"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenising Text"
      ],
      "metadata": {
        "id": "p1Qm6MogAm44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJP9Xpyn_-Aa",
        "outputId": "3dbf3acb-df13-44ab-f92d-d142ddbb339b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULeG7APLA2hi",
        "outputId": "f907377f-9764-4155-8bcd-b152b07b5555"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Initialise dummy GPT model and feed it tokenized batch\n",
        "'''The output tensor has two rows corresponding to the two text samples. Each text sample consists of four tokens; each token is a 50,257-dimensional vector\n",
        "This represents one score per vocabulary token\n",
        "'''\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "print(\"Output shape:\", logits.shape)\n",
        "print(logits)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADu14mAxA3Vm",
        "outputId": "a75058ff-a543-4b75-d559-e98bf0ad4b68"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden representation shape: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
            "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
            "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
            "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
            "\n",
            "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
            "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
            "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
            "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.shape"
      ],
      "metadata": {
        "id": "mNF8hTwBBAdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5hCvfqxOBZw",
        "outputId": "d4098a81-fb76-411d-b4de-473f99439365"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6109, 3626, 6100,  345],\n",
              "        [6109, 1110, 6622,  257]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Tokeniser creates a batch of 2 with 4 tokens in\n",
        "##This is then passed to embedding layers as part of GPTModel\n",
        "# Each text sample consists of 4 tokens; each token is a 50257 dimensional vector matching tiktoken's bpe config\n",
        "'''\n",
        "token ids\n",
        "   ↓\n",
        "embedding lookup\n",
        "   ↓\n",
        "(batch, seq, 768)   ← internal representation\n",
        "   ↓\n",
        "linear projection\n",
        "   ↓\n",
        "(batch, seq, 50257) ← logits\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gr_AYHYsBh9Y",
        "outputId": "18d61b0b-13db-4619-9382-f70d18ef9696"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntoken ids\\n   ↓\\nembedding lookup\\n   ↓\\n(batch, seq, 768)   ← internal representation\\n   ↓\\nlinear projection\\n   ↓\\n(batch, seq, 50257) ← logits\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''This is the embedding matrix. A row per token and a column per dimension'''\n",
        "print(model.tok_emb.weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpBpG_PHDc0m",
        "outputId": "56d9e635-f215-40b1-9406-7526030035e1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thinking about the shape of the inputs and outputs\n",
        "\n",
        "One of the slightly confusing elements of the book is how this is stated:\n",
        "\n",
        "> The output tensor has two rows corresponding to the two text samples. Each text sample consists of four tokens; each token is a 50,257-dimensional vector, which matches the size of the tokenizer’s vocabulary.\n",
        "\n",
        "> The embedding has 50,257 dimensions because each of these dimensions refers to a unique token in the vocabulary. When we implement the postprocessing code, we will convert these 50,257-dimensional vectors back into token IDs, which we can then decode into words.\n",
        "\n",
        "It can be a bit difficult to follow this, we know that we go from a 768 dimension embedding with a vocab size of 50,257, but it's not really clear what's happening to then get to the logit out.\n",
        "\n",
        "From working through the logic, we use a linear layer to go from a 768-dim embedding to 50,257 dimension of token scores.\n",
        "\n",
        "When we train the model, we'll use the 768 dimension semantic space, but we'll then 'predict' by looking at the token scores across a 50,257 dimension.\n",
        "\n",
        "A good tip to remember that for this GPT-2 example, the weight matrix (embeddings) are:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "print(model.tok_emb.weight.shape)\n",
        "```\n",
        "which is 50,257 rows (one row per possible token) and 768 columns (one column per embedding dimension)\n",
        "\n",
        "Adapting from Chapter 3:\n",
        "\n",
        "> \"In other words, the embedding layer is essentially a lookup operation that retrieves rows from the embedding layer’s weight matrix via a token ID.\"\n",
        "\n",
        "The embedding dimension \"represents the embedding size, transforming each token into a 768-dimensional vector\"\n",
        "\n",
        "Therefore, we create a 50,257 x 768 matrix. This then goes through a linear layer to output 'logits'. In the batch example, this has two rows corresponding to the two text samples. Each text sample consists of four tokens; each token is a 50,257-dimensional vector, which matches the size of the tokenizer's vocabulary.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EFvr7Z8bQy_j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RgZHvep_DdGO"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}